1. Now lets verify the health checks. We need the IP address of the agent pod. You could use something like this: `k get pods -o wide`{{execute}} or if you enjoy using jq and want just the pod name and ip address, try this command: `k get pod -o json | jq -r '.items[] | .metadata.name + " - IP: " + .status.podIPs[].ip '`{{execute}}. A great resource for learning more about manipulating the output of kubectl with tools like jq is https://gist.github.com/so0k/42313dbb3b547a0f51a547bb968696ba. 
1. Review values.yaml around line 200. You can see this is where the liveness probe is defined for the agent. Now run `curl <pod ip address>:5555/health`, replacing <pod ip address> with the address you found above. Pipe the output to `jq` to make the output a bit easier to read (`curl <pod ip address>:5555/health | jq`).
1. Try going back to the low memory and cpu settings above and trying the healthcheck again. You will see more items in the unhealthy block and few in the healthy block if any at all. When done, return the memory and cpu settings again to the original good values. 
1. Run the Helm upgrade command again: `helm upgrade datadogagent --set datadog.apiKey=$DD_API_KEY --set datadog.appKey=$DD_APP_KEY -f k8s-yaml-files/values.yaml datadog/datadog`{{execute}}
1. Finally you can verify the status of the Datadog Agent the same way you do it on other platforms. Just run `kubectl exec -ti $(kubectl get pods -l app=datadogagent -o jsonpath='{.items[0].metadata.name}') -- agent status`{{execute}}. Notice that this is injecting the agent pod into the command automatically. 

When scanning through the values.yaml file, you may have noticed a section named clusteragent. We will learn more about that in the next section. Click the continue button to move on. 